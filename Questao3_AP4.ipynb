{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Alunos\n",
        "\n",
        "| Nome                        | Matrícula |\n",
        "|-----------------------------|-----------|\n",
        "| Luiz Filipe Bartelega Penha | 202111082 |\n",
        "| Vitor Pires Zini            | 202110169 |"
      ],
      "metadata": {
        "id": "51Vgw-bPQksQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo de Programação Dinâmica para Estratégias Ótimas no Jogo \"Quem Quer Ser um Milionário\"\n",
        "\n",
        "Neste problema, buscamos modelar o jogo Quem Quer Ser um Milionário? utilizando programação dinâmica para determinar estratégias ótimas que maximizem dois objetivos diferentes:\n",
        "\n",
        "1) A recompensa esperada acumulada ao longo do jogo\n",
        "\n",
        "2) A probabilidade de alcançar e responder corretamente a uma pergunta específica.\n",
        "\n",
        "A solução envolve calcular, para cada estado do jogo (pergunta atual e ajudas disponíveis), as melhores decisões, considerando o uso ou não de ajudas e o impacto das probabilidades de sucesso."
      ],
      "metadata": {
        "id": "u--jj85wxYUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resolução via Programação Dinâmica\n",
        "\n",
        "#### Modelo de programação dinâmica\n",
        "\n",
        "O modelo apresentado busca determinar as melhores decisões em cada etapa do jogo para atingir os dois objetivos descritos anteriormente.\n",
        "\n",
        "Para isso, o modelo considera as recompensas associadas a cada pergunta, os riscos de errar e o impacto das ajudas disponíveis. A solução é calculada de forma recursiva, analisando todos os estados possíveis e as transições entre eles.\n",
        "\n",
        "O modelo de programação dinâmica é apresentado abaixo.\n",
        "\n",
        "* Estágios: cada estágio representa uma pergunta do jogo, numerada de 1 a num_perguntas. A lógica do modelo trabalha de trás para frente, ou seja, começa na última pergunta e calcula recursivamente o valor ótimo de cada estágio até a primeira pergunta.\n",
        "\n",
        "* Estados: Um estado é representado pela pergunta atual k e pela disponibilidade das ajudas (ex: plateia, 50:50, telefone). As combinações das ajudas são modeladas como tuplas binárias, onde 1 significa que a ajuda está disponível, e 0 que já foi usada.\n",
        "\n",
        "* Função de transição:\n",
        "\n",
        "\\begin{equation}\n",
        "H(k,s) = \\max\\left\\{r_k, max [p^s_a . H(k + 1, t(s,a)) + (1 - p^s_a) . r^´_k])\\right\\}\n",
        "\\end{equation}\n",
        "\n",
        "Onde:\n",
        "\n",
        "* r_k: Recompensa para desistir na pergunta k.\n",
        "* A(s): Conjunto de todas as ações possíveis (combinações de uso das ajudas).\n",
        "* P^s_a:  Probabilidade de sucesso ao tomar a ação \"a\" no estado \"s\".\n",
        "* H(k+1,t(s,a)): Valor esperado no próximo estágio ao acertar a pergunta k.\n",
        "* R'k: Recompensa ao errar a pergunta k.\n",
        "\n",
        "Probabilidade de alcançar uma pergunta específica:\n",
        "\n",
        "\\begin{equation}\n",
        "P(k,s) = \\max\\left\\{[p^s_a . P(k + 1, t(s,a))\\right\\} \\forall a \\in A(s)\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "c6Tcxo25x_Q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Definição dos inputs\n",
        "\n",
        "* num_perguntas: Número total de perguntas do jogo. Cada pergunta corresponde a um estágio no modelo. No exemplo, temos 15 perguntas.\n",
        "\n",
        "* ajudas: Lista de ajudas disponíveis no jogo, como \"plateia\", \"50:50\" e \"telefone\". Isso permite modelar o impacto dessas opções nas probabilidades de sucesso.\n",
        "\n",
        "* recompensas: Lista com os valores monetários atribuídos a cada pergunta. Por exemplo, a recompensa para a pergunta 1 é 100 e para a última é 1.000.000.\n",
        "\n",
        "* recompensas_erro: Lista com os valores garantidos em caso de erro. Esses valores geralmente correspondem aos \"pontos de segurança\" do jogo, como 1000 ou 32000, que são preservados mesmo ao errar.\n",
        "\n",
        "* probabilidades: Um dicionário que define:\n",
        "\n",
        "  * base: Probabilidades de sucesso para cada pergunta, sem usar ajudas.\n",
        "\n",
        "  * plateia, 50:50, telefone: Modificadores que aumentam as probabilidades de sucesso ao usar essas ajudas. Por exemplo, \"plateia\" multiplica a probabilidade base por 1.1.\n",
        "\n"
      ],
      "metadata": {
        "id": "nnxEVnEp2Kt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementação\n"
      ],
      "metadata": {
        "id": "Fv0qpmYyinTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemente seu modelo na sequência. **A solução ótima é dada por $2$**, correspondente à subárvore $0,1,4,8,9$."
      ],
      "metadata": {
        "id": "IN5dCUYx2Ps4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class JogoMilionarioDP:\n",
        "    def __init__(self, num_perguntas, ajudas, recompensas, recompensas_erro, probabilidades):\n",
        "        \"\"\"\n",
        "        Inicializa os parâmetros do modelo de programação dinâmica.\n",
        "\n",
        "        Args:\n",
        "            num_perguntas (int): Número total de perguntas no jogo.\n",
        "            ajudas (list): Lista de ajudas disponíveis (ex.: ['plateia', '50:50', 'telefone']).\n",
        "            recompensas (list): Lista de recompensas para cada pergunta.\n",
        "            recompensas_erro (list): Recompensas garantidas ao errar.\n",
        "            probabilidades (dict): Probabilidades de sucesso por pergunta e uso de ajudas.\n",
        "        \"\"\"\n",
        "        self.num_perguntas = num_perguntas\n",
        "        self.ajudas = ajudas\n",
        "        self.recompensas = recompensas\n",
        "        self.recompensas_erro = recompensas_erro\n",
        "        self.probabilidades = probabilidades\n",
        "\n",
        "    def maximizar_recompensa_esperada(self):\n",
        "        \"\"\"Calcula a recompensa esperada máxima usando programação dinâmica.\"\"\"\n",
        "        # Criação de uma matriz DP para armazenar os valores de H(v)\n",
        "        dp = {}\n",
        "\n",
        "        # Itera de trás para frente (da última pergunta até a primeira)\n",
        "        for k in range(self.num_perguntas, 0, -1):\n",
        "            for estado in self._gerar_estados():\n",
        "                if k == self.num_perguntas:\n",
        "                    # Última pergunta: calcular diretamente com as probabilidades disponíveis\n",
        "                    dp[(k, estado)] = self._calcular_recompensa_terminal(k, estado)\n",
        "                else:\n",
        "                    dp[(k, estado)] = self._calcular_recompensa_recursiva(k, estado, dp)\n",
        "\n",
        "        # Retorna a recompensa máxima esperada a partir da primeira pergunta\n",
        "        return dp[(1, self._estado_inicial())]\n",
        "\n",
        "    def maximizar_probabilidade_alcancar_pergunta(self, pergunta_alvo):\n",
        "        \"\"\"Maximiza a probabilidade de alcançar uma pergunta específica.\"\"\"\n",
        "        dp = {}\n",
        "\n",
        "        # Itera de trás para frente (da pergunta_alvo até o início)\n",
        "        for k in range(pergunta_alvo, 0, -1):\n",
        "            for estado in self._gerar_estados():\n",
        "                if k == pergunta_alvo:\n",
        "                    # Alcançar a pergunta-alvo\n",
        "                    dp[(k, estado)] = 1\n",
        "                else:\n",
        "                    dp[(k, estado)] = self._calcular_probabilidade_recursiva(k, estado, dp)\n",
        "\n",
        "        return dp[(1, self._estado_inicial())]\n",
        "\n",
        "    def _gerar_estados(self):\n",
        "        \"\"\"Gera combinações possíveis de estados (ajudas disponíveis).\"\"\"\n",
        "        from itertools import product\n",
        "        return list(product([0, 1], repeat=len(self.ajudas)))\n",
        "\n",
        "    def _estado_inicial(self):\n",
        "        \"\"\"Estado inicial: todas as ajudas disponíveis.\"\"\"\n",
        "        return tuple([1] * len(self.ajudas))\n",
        "\n",
        "    def _calcular_recompensa_terminal(self, k, estado):\n",
        "        \"\"\"Calcula a recompensa esperada para o estado terminal.\"\"\"\n",
        "        return max(self.recompensas[k - 1], self.recompensas_erro[k - 1])\n",
        "\n",
        "    def _calcular_recompensa_recursiva(self, k, estado, dp):\n",
        "        \"\"\"Calcula a recompensa esperada para um estado intermediário.\"\"\"\n",
        "        max_recompensa = self.recompensas[k - 1]  # Caso de desistência\n",
        "\n",
        "        # Testar todas as ações possíveis\n",
        "        for acao_ajuda in self._gerar_acoes_ajudas(estado):\n",
        "            prob_sucesso = self._obter_probabilidade(k, acao_ajuda)\n",
        "            proximo_estado = self._transicao(estado, acao_ajuda)\n",
        "\n",
        "            recompensa = (prob_sucesso * dp[(k + 1, proximo_estado)] +\n",
        "                          (1 - prob_sucesso) * self.recompensas_erro[k - 1])\n",
        "            max_recompensa = max(max_recompensa, recompensa)\n",
        "\n",
        "        return max_recompensa\n",
        "\n",
        "    def _calcular_probabilidade_recursiva(self, k, estado, dp):\n",
        "        \"\"\"Calcula a probabilidade máxima de alcançar a pergunta-alvo.\"\"\"\n",
        "        max_prob = 0\n",
        "\n",
        "        for acao_ajuda in self._gerar_acoes_ajudas(estado):\n",
        "            prob_sucesso = self._obter_probabilidade(k, acao_ajuda)\n",
        "            proximo_estado = self._transicao(estado, acao_ajuda)\n",
        "            max_prob = max(max_prob, prob_sucesso * dp[(k + 1, proximo_estado)])\n",
        "\n",
        "        return max_prob\n",
        "\n",
        "    def _gerar_acoes_ajudas(self, estado):\n",
        "        \"\"\"Gera combinações de uso de ajudas com base no estado atual.\"\"\"\n",
        "        from itertools import product\n",
        "        return list(product([0, 1], repeat=len(self.ajudas)))\n",
        "\n",
        "    def _obter_probabilidade(self, k, acao_ajuda):\n",
        "        \"\"\"Obtém a probabilidade de sucesso para uma ação específica.\"\"\"\n",
        "        prob_base = self.probabilidades['base'][k - 1]\n",
        "        modificador = 1\n",
        "        for i, uso in enumerate(acao_ajuda):\n",
        "            if uso:\n",
        "                modificador *= self.probabilidades[self.ajudas[i]][k - 1]\n",
        "        return prob_base * modificador\n",
        "\n",
        "    def _transicao(self, estado, acao_ajuda):\n",
        "        \"\"\"Transiciona para o próximo estado com base na ação atual.\"\"\"\n",
        "        return tuple(max(0, estado[i] - acao_ajuda[i]) for i in range(len(estado)))\n",
        "\n",
        "# Exemplo de uso do modelo\n",
        "jogo = JogoMilionarioDP(\n",
        "    num_perguntas=15,\n",
        "    ajudas=['plateia', '50:50', 'telefone'],\n",
        "    recompensas=[100, 200, 300, 500, 1000, 2000, 4000, 8000, 16000, 32000, 64000, 125000, 250000, 500000, 1000000],\n",
        "    recompensas_erro=[0, 0, 0, 0, 1000, 1000, 1000, 1000, 1000, 32000, 32000, 32000, 32000, 32000, 32000],\n",
        "    probabilidades={\n",
        "        'base': [0.9, 0.85, 0.8, 0.75, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2],\n",
        "        'plateia': [1.1] * 15,\n",
        "        '50:50': [1.2] * 15,\n",
        "        'telefone': [1.15] * 15\n",
        "    }\n",
        ")\n",
        "\n",
        "max_recompensa = jogo.maximizar_recompensa_esperada()\n",
        "max_prob = jogo.maximizar_probabilidade_alcancar_pergunta(15)\n",
        "\n",
        "print(f\"Recompensa esperada máxima: {max_recompensa}\")\n",
        "print(f\"Probabilidade máxima de alcançar a última pergunta: {max_prob}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvph4k9p5Om3",
        "outputId": "ced327dd-dbf1-4302-d1f1-518c24c95ad1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recompensa esperada máxima: 119012.56132889456\n",
            "Probabilidade máxima de alcançar a última pergunta: 0.056171321470692794\n"
          ]
        }
      ]
    }
  ]
}